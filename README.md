<div style="text-align: center;">

# Code Generation Models

</div>

## Overview:

This experiment is mainly to experiment with domain specific pretraining in the context of code generation language models.

## This project has been abandoned in favour of a JAX implementation 


## Model used:
StableLM Base Alpha 3B

## Dataset used:
The Pile (StableLM Base Alpha 3B) for pretrained model

CodeParrot Clean for both

#### Unconfirmed (Planning)

Arxiv CS papers and other code related datasets without actual unlabelled code (for custom pretraining)

## Project underway

Code and benchmarks coming in the near future

## Credits

Thanks to the [TPU Research Cloud (TRC) program](https://sites.research.google/trc/about/) for providing compute resources

Thanks to the various authors whose code I have used to build this. Most notably, [Lit-GPT by Lightning AI](https://github.com/Lightning-AI/lit-gpt).

## Contact

Please reach out to me at at my [email](mailto:221474B@student.hci.edu.sg) to contact me.
